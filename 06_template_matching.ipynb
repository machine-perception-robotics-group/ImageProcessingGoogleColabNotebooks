{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"06_template_matching","provenance":[{"file_id":"https://github.com/machine-perception-robotics-group/ImageProcessingGoogleColabNotebooks/blob/master/06_template_matching.ipynb","timestamp":1625666601372}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"844HprfC2ezF"},"source":["# 06. テンプレートマッチング\n","講義で説明する画像処理の方法について，google colaboratoryを利用して演習する．\n","google colaboratoryは，クラウドで実行する Jupyter ノートブック環境である.\n","google coraboratoryについては，[ここ](https://www.tdi.co.jp/miso/google-colaboratory-gpu)や[ここ](https://www.codexa.net/how-to-use-google-colaboratory/)を参考にすること．\n","\n","下記のプログラムを実行すると，様々な類似尺度を用いたテンプレートマッチングや高速化のための粗密探索を実行する．"]},{"cell_type":"markdown","metadata":{"id":"7Bbaet2C4U7F"},"source":["## 準備\n","プログラムの動作に必要なデータをダウンロードし，zipファイルを解凍する．\n","`!`で始まるコマンドはPythonではなく，Linux（Ubuntu）のコマンドを実行している．"]},{"cell_type":"code","metadata":{"id":"3NYtuZtndzQ4"},"source":["!wget -q http://www.mprg.cs.chubu.ac.jp/Tutorial/ML_Lecture/tutorial_ip_2020/image1.zip\n","!unzip -q image1.zip\n","!ls\n","!ls ./image1/"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mkwrQ8dwa2sZ"},"source":["## 画像の読み込みと表示\n","必要なパッケージをインポートし，画像を表示する．"]},{"cell_type":"code","metadata":{"id":"YFl5XkzmZCg9"},"source":["import math\n","import cv2\n","import numpy as np\n","from matplotlib import pyplot as plt\n","\n","img1 = cv2.imread('./image1/woman-t.jpg', 0)\n","img2 = cv2.imread('./image1/woman-g.jpg', 0)\n","\n","plt.imshow(img1, cmap=\"gray\", clim=(0, 255))\n","plt.title(\"template\")\n","plt.show()\n","\n","plt.imshow(img2, cmap=\"gray\", clim=(0, 255))\n","plt.title(\"original\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bNRF8z1MGlOT"},"source":["## 類似尺度\n","\n","テンプレートマッチングでは，テンプレートと入力画像の画素比較によって類似度を算出する．類似度の高い位置を算出することで入力画像中のテンプレート画像位置を探索できる．\n","\n","テンプレートマッチングで使用する類似尺度は様々なものがある．\n","ここでは，以下の類似尺度を使用してテンプレートマッチングを行う．\n","\n","* Sum of Squared Difference (SSD)\n","* Sum of Absolute Difference (SAD)\n","* 正規化相互相関（Normalized Cross-Correlation）\n","* ゼロ平均相互相関係数（Zero-means Normalized Cross-Correlation）\n","\n","まず，はじめに類似尺度を計算するための関数を定義する．\n","ここでは，テンプレートと類似度を計算したい画像の局所領域を入力した際に，その類似度を計算して返すような関数を定義する．\n","\n","全ての関数において，入力された画像は`flatten`関数によって2次元配列から1次元配列へと並び替えられる（`ravel`と同様の効果）．\n","\n","その後，各尺度の定義に合わせて類似度を計算する．\n","ここでは，Numpy配列の演算を利用して類似度計算を行っている．\n","Numpy配列の演算では，配列の各要素に対して同一の演算を行うことが可能となる．\n","例えば，`img1 - img2`では，`img1`の各要素と`img2`の各要素の減算を行なっており，`diff**2`では各要素の2乗を計算している．\n","Numpy配列の演算を用いることで，これまでに行なってきたfor文で一つづつ計算するよりも高速に計算することが可能となる．"]},{"cell_type":"code","metadata":{"id":"AZBE-fPcHhGA"},"source":["def ssd(img1, img2):\n","  img1 = np.float32(img1.flatten())\n","  img2 = np.float32(img2.flatten())\n","\n","  diff = img1 - img2\n","  ssd_val = np.sum(diff**2)\n","  return ssd_val\n","\n","  \n","def sad(img1, img2):\n","  img1 = np.float32(img1.flatten())\n","  img2 = np.float32(img2.flatten())\n","  \n","  sad_val = np.sum(np.abs(img1 - img2))\n","  return sad_val\n","\n","\n","def norm_cross_correlation(img1, img2):\n","  img1 = np.float32(img1.flatten())\n","  img2 = np.float32(img2.flatten())\n","  \n","  sum1 = np.sum(img1**2)\n","  sum2 = np.sum(img2**2)\n","  sum_prod = np.sum(img1 * img2)\n","  ncc_val = sum_prod / math.sqrt(sum1 * sum2)\n","  return ncc_val\n","\n","\n","def zeromean_norm_cross_correlation(img1, img2):\n","  img1 = np.float32(img1.flatten())\n","  img2 = np.float32(img2.flatten())\n","\n","  mean1 = np.mean(img1)\n","  mean2 = np.mean(img2)\n","\n","  subtract_mean1 = img1 - mean1\n","  subtract_mean2 = img2 - mean2\n","\n","  sum1 = np.sum(subtract_mean1**2)\n","  sum2 = np.sum(subtract_mean2**2)\n","\n","  sum_prod = np.sum(subtract_mean1 * subtract_mean2)\n","\n","  zm_ncc_val = sum_prod / math.sqrt(sum1 * sum2)\n","  return zm_ncc_val"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-01hn-eVe3JF"},"source":["次に，指定した画像とテンプレートでテンプレートマッチングを行うための関数`matching_template`を定義する．\n","\n","この関数では，探索したい元画像とテンプレートを引数として入力する．\n","また，第3引数の`metric`は，使用する類似尺度を指定する．\n","`matching_template`関数では，上で定義した類似尺度の**関数を引数として入力**する．これにより関数中の`metric(...)`において，引数で指定した類似尺度を計算するための関数を用いて類似度計算を行うことができる．\n","第4引数はもっとも類似度が高い座標を求めるために，最小値（最大値）を探すかどうかを指定する引数である．\n","類似尺度によって，最も低い（最も高い）箇所が探索結果となる．\n","そのため，第3引数で指定した類似尺度に合わせて最小値または最大値の座標のどちらを求めるかを指定する．\n","`find_min=True`となっているのは，デフォルト引数であり．関数を使用する際に，この引数を記述しない場合は，`True`がデフォルト値として使用されることを示している．\n","\n","画像，テンプレート，尺度等を指定したのち，テンプレートマッチングを行う．\n","\n","まず，類似度を保存するための配列`template_socre`を用意する．\n","その後，for文で1領域ごとに類似度を計算し，その結果を保存する．\n","類似度の計算が全ての領域で終わると，`find_min`に従って，類似度が最小値または最大値の座標を求める．\n","`argmin`関数は要素の値が最小となる箇所のインデックスを計算する関数であるが，通常は1次元配列に対して使用するものであり．2次元配列に対するインデックスを計算することができない．\n","ここでは`unravel_index`関数を用いることで，2次元配列のまま最小値（最大値）のインデックスを計算している．\n","\n","得られた座標は左上の座標であり，領域を指定するため，右下の座標をテンプレートサイズから計算し，\n","\n","1. 探索した類似度のマップ\n","2. マッチング結果の左上座標\n","3. マッチング結果の右下座標\n","\n","を返す．"]},{"cell_type":"code","metadata":{"id":"q36ea5kYUadG"},"source":["def matching_template(img, template, metric, find_min=True):\n","  h, w = img.shape\n","  ht, wt = template.shape\n","  template_score = np.zeros([h - ht, w - wt], dtype=np.float32)\n","\n","  for y in range(h - ht):\n","    for x in range(w - wt):\n","      template_score[y, x] = metric(img[y:y+ht, x:x+wt], template)\n","  \n","  if find_min:\n","    top_left = np.unravel_index(np.argmin(template_score), template_score.shape)\n","  else:\n","    top_left = np.unravel_index(np.argmax(template_score), template_score.shape)\n","\n","  bottom_right = (top_left[0] + ht, top_left[1] + wt)\n","  return template_score, top_left[::-1], bottom_right[::-1]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SoC-i7J1KzTd"},"source":["### Sum of Squared Difference (SSD)\n","\n","SSDを類似尺度として用いた場合のテンプレートマッチングを行う．\n","テンプレートマッチングの処理は，すでに上の関数で全て定義してあるため．`matching_template`関数の第3引数を`ssd`と指定することで計算が可能である．\n","また，SSDは値が小さい場合に類似度が高くなるため，第4引数の`find_min`をTrueにする必要があるが，デフォルト値ですでに`True`が指定されているため，記述していない．\n","\n","マッチング結果が得られたら，`rectangle`関数を用いて矩形を描画し，結果を表示する．\n","\n"]},{"cell_type":"code","metadata":{"id":"3d39pgKUUmdP"},"source":["img = img2.copy()\n","template = img1.copy()\n","\n","ssd_score, top_left_coord, bottom_right_coord = matching_template(img, template, ssd)\n","\n","print(\"found coordinate (x, y)\")\n","print(\"   top-left    :\", top_left_coord)\n","print(\"   bottom-right:\", bottom_right_coord)\n","\n","cv2.rectangle(img, top_left_coord, bottom_right_coord, 255, 5)\n","\n","plt.figure(figsize=(12, 4))\n","plt.subplot(121), plt.imshow(ssd_score, cmap=\"jet\")\n","plt.colorbar()\n","plt.subplot(122), plt.imshow(img, cmap=\"gray\", clim=(0, 255))\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-MAxnhN4VnMg"},"source":["### Sum of Absolute Difference (SAD)\n","\n","SADもSDDと同様の手順でテンプレートマッチングを行う．\n","そのため，`matching_template`関数の第3引数を`sad`に変更するだけで異なる類似尺度での探索が可能である．\n","\n"]},{"cell_type":"code","metadata":{"id":"VOGA3dR_VnRa"},"source":["img = img2.copy()\n","template = img1.copy()\n","\n","sad_score, top_left_coord, bottom_right_coord = matching_template(img, template, sad)\n","\n","print(\"found coordinate (x, y)\")\n","print(\"   top-left    :\", top_left_coord)\n","print(\"   bottom-right:\", bottom_right_coord)\n","\n","cv2.rectangle(img, top_left_coord, bottom_right_coord, 255, 5)\n","\n","plt.figure(figsize=(12, 4))\n","plt.subplot(121), plt.imshow(sad_score, cmap=\"jet\")\n","plt.colorbar()\n","plt.subplot(122), plt.imshow(img, cmap=\"gray\", clim=(0, 255))\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J0AZ0cB1VnV_"},"source":["### 正規化相互相関（Normalized Cross-Correlation）\n","\n","正規化相互相関も，SSDおよびSADと同様の手順で実行することが可能であるが，\n","この類似尺度では最も値が高い箇所をマッチング結果として探索する必要があるため，第4引数を`find_min=False`と指定して，最大値を探すようにする必要がある．\n"]},{"cell_type":"code","metadata":{"id":"BGlASLJwVnaY"},"source":["img = img2.copy()\n","template = img1.copy()\n","\n","ncc_score, top_left_coord, bottom_right_coord = matching_template(img, template, norm_cross_correlation, find_min=False)\n","\n","print(\"found coordinate (x, y)\")\n","print(\"   top-left    :\", top_left_coord)\n","print(\"   bottom-right:\", bottom_right_coord)\n","\n","cv2.rectangle(img, top_left_coord, bottom_right_coord, 255, 5)\n","\n","plt.figure(figsize=(12, 4))\n","plt.subplot(121), plt.imshow(ncc_score, cmap=\"jet\")\n","plt.colorbar()\n","plt.subplot(122), plt.imshow(img, cmap=\"gray\", clim=(0, 255))\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WaP-e0RgVnfH"},"source":["### ゼロ平均相互相関係数（Zero-means Normalized Cross-Correlation）\n","\n","ゼロ平均相互相関係数についても，正規化相互相関と同様の手順で実行が可能である．\n","\n","ゼロ平均相互相関係数の尺度を用いた場合．正規化相互相関よりもマッチングする箇所とそうでない箇所の類似度の差が大きいことが確認できる．\n","\n"]},{"cell_type":"code","metadata":{"id":"iwXmt9ryVnjs"},"source":["img = img2.copy()\n","template = img1.copy()\n","\n","zm_ncc_score, top_left_coord, bottom_right_coord = matching_template(img, template, zeromean_norm_cross_correlation, find_min=False)\n","\n","print(\"found coordinate (x, y)\")\n","print(\"   top-left    :\", top_left_coord)\n","print(\"   bottom-right:\", bottom_right_coord)\n","\n","cv2.rectangle(img, top_left_coord, bottom_right_coord, 255, 5)\n","\n","plt.figure(figsize=(12, 4))\n","plt.subplot(121), plt.imshow(zm_ncc_score, cmap=\"jet\")\n","plt.colorbar()\n","plt.subplot(122), plt.imshow(img, cmap=\"gray\", clim=(0, 255))\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p2SibhTva8GC"},"source":["## 粗密探索\n","\n","テンプレートマッチングは探索する画像の大きさ等に応じて処理時間が増加する．\n","そのため，大きな画像に対してマッチングを行う場合には，効率的な探索を行う必要がある．\n","\n","効率的なテンプレートマッチングの方法として粗密探索がある．\n","粗密探索では，はじめに，低解像度の小さな画像で探索を行う．\n","その結果得られた座標の近傍のみで，高解像度な探索を行うことで効率化を行う方法である．\n","\n","まずはじめに，従来の全探索（全ての領域で類似度計算を行う方法）の処理時間を計測する．\n","時間の計測には`time`モジュールの`time`関数（`time.time()`）使用する．\n","`time`関数は，この関数が実行された際の時刻を保存する関数である．\n","そのため，テンプレートマッチングの開始前後の時刻を記録して，その差分を計算することで処理時間を計測することが可能となる．\n","\n","テンプレートマッチングの実行には，4つの類似尺度のうち，最も計算時間がかかると思われる，ゼロ平均相互相関係数を使用する．\n"]},{"cell_type":"code","metadata":{"id":"Jo7vh3lua8Kw"},"source":["import time\n","\n","img = img2.copy()\n","template = img1.copy()\n","\n","time_start1 = time.time()\n","zm_ncc_score, top_left_coord, bottom_right_coord = matching_template(img, template, zeromean_norm_cross_correlation, find_min=False)\n","time_end1 = time.time()\n","print(\"found top-left coordinate:\", top_left_coord)\n","print(\"processing time:\", time_end1 - time_start1, \"[s]\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bQ9y2PekVgaN"},"source":["次に粗密探索によるテンプレートマッチングを実行し，処理時間を計測する．\n","\n","はじめに，前準備として，低解像度の画像とテンプレート（`img_small`, `template_small`）を用意する．\n","ここでは，元のサイズの1/4のサイズにリサイズする．\n","\n","`time`関数で時間を計測し，低解像度画像でテンプレートマッチングを行う．\n","その後，低解像度画像で得られた結果に対応する領域を元画像から抽出する．\n","そして，対応領域のみを用いてテンプレートマッチングを行い，得られた結果を元画像の座標へと変換してマッチングを終了する．\n","\n","実行の結果，上で実行した全探索に比べて大幅に処理時間を削減できていることがわかる．\n"]},{"cell_type":"code","metadata":{"id":"tFIRjc1siWci"},"source":["img = img2.copy()\n","template = img1.copy()\n","ht, wt = template.shape\n","\n","# 画像を1/4のサイズにリサイズ\n","img_small = cv2.resize(img, None, fx=0.25, fy=0.25)\n","template_small = cv2.resize(template, None, fx=0.25, fy=0.25)\n","\n","time_start2 = time.time()\n","# 低解像度画像でテンプレートマッチング\n","score_s, tl_s, br_s = matching_template(img_small, template_small, zeromean_norm_cross_correlation, find_min=False)\n","print(\"found top-left coordinate (small)   :\", tl_s)\n","\n","# 低解像度画像に対応する領域を元画像から抽出\n","img_trimmed = img[tl_s[0]*4:(tl_s[0]+1)*4+ht, tl_s[1]*4:(tl_s[1]+1)*4+wt]\n","\n","# 対応領域を抽出した画像でテンプレートマッチング\n","score, tl, br = matching_template(img_trimmed, template, zeromean_norm_cross_correlation, find_min=False)\n","print(\"found top-left coordinate (trimmed) :\", tl)\n","\n","# 得られた座標を元画像中の座標に変換\n","top_left = (tl[0] + tl_s[0]*4, tl[1] + tl_s[1]*4)\n","bottom_right = (br[0] + tl_s[0]*4, br[1] + tl_s[1]*4)\n","print(\"found top-left coordinate (original):\", top_left)\n","time_end2 = time.time()\n","print(\"processing time:\", time_end2 - time_start2, \"[s]\")\n","\n","# 結果の描画\n","cv2.rectangle(img_small, tl_s, br_s, 255, 1)\n","cv2.rectangle(img, top_left, bottom_right, 255, 5)\n","\n","# 表示\n","plt.figure(figsize=(12, 4))\n","plt.subplot(121), plt.imshow(score_s, cmap=\"jet\")\n","plt.colorbar()\n","plt.subplot(122), plt.imshow(img_small, cmap=\"gray\", clim=(0, 255))\n","plt.show()\n","\n","plt.figure(figsize=(12, 4))\n","plt.subplot(121), plt.imshow(score, cmap=\"jet\")\n","plt.colorbar()\n","plt.subplot(122), plt.imshow(img, cmap=\"gray\", clim=(0, 255))\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"svRqPEAyJCqM"},"source":["## OpenCVを用いた方法\n","\n","OpenCVにはテンプレートマッチングを行うための，`matchTemplate`関数がある．\n","この関数では，いくつかの類似尺度を使用することができる．\n","\n","以下では，`matchTemplate`関数を用いて，それぞれの類似尺度を用いた場合の結果を示す．\n","なお，それぞれの類似尺度の詳細については[OpenCVのドキュメント](https://docs.opencv.org/master/df/dfb/group__imgproc__object.html)を参照すること．"]},{"cell_type":"code","metadata":{"id":"FGIsqeqtfmu0"},"source":["template = img1.copy()\n","h, w = template.shape\n","\n","# 6つの方法で比較を行う\n","methods = ['cv2.TM_SQDIFF',         # 差分相関 (SSD)\n","           'cv2.TM_SQDIFF_NORMED',  # 正規化差分相関\n","           'cv2.TM_CCORR',          # 相互相関\n","           'cv2.TM_CCORR_NORMED',   # 正規化相互相関\n","           'cv2.TM_CCOEFF',         # 相関係数\n","           'cv2.TM_CCOEFF_NORMED']  # 正規化相関係数\n","\n","# 上で定義した方法を一つづつ実行\n","for meth in methods:\n","  img = img2.copy()\n","  method = eval(meth)\n","\n","  # テンプレートマッチング\n","  res = cv2.matchTemplate(img, template, method)\n","  min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(res)\n","\n","  # メソッドが TM_SQDIFF または TM_SQDIFF_NORMED の場合は，最小値を取る\n","  if method in [cv2.TM_SQDIFF, cv2.TM_SQDIFF_NORMED]:\n","    top_left = min_loc\n","  else:\n","    top_left = max_loc\n","  bottom_right = (top_left[0] + w, top_left[1] + h)\n","\n","  cv2.rectangle(img,top_left, bottom_right, 255, 5)\n","\n","  plt.figure(figsize=(12, 4))\n","  plt.subplot(121), plt.imshow(res, cmap='jet')\n","  plt.title('Matching Score')\n","  plt.colorbar()\n","  plt.subplot(122), plt.imshow(img, cmap='gray', clim=(0, 255))\n","  plt.title('Detected Point')\n","  plt.suptitle(meth)\n","\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_584L4TdT6wB"},"source":["## チャンファーマッチング (Chamfer Matching)\n","\n","チャンファーマッチング (Chamfer Matching) は，入力画像とテンプレート画像間のエッジの相違度に基づいて探索を行う手法である．\n","\n","チャンファーマッチングでは，まず画像のエッジを抽出し，エッジ画像に距離変換処理を行い距離変換画像を作成する．この距離変換画像ではエッジの距離を0とし，エッジから離れるほど距離が増加する．入力画像から作成した距離変換画像とテンプレート画像から作成したエッジ画像を相違度に基づいていマッチングを行う．\n","\n","まずはじめに，入力画像とテンプレート画像のエッジ画像を作成する．エッジ検出法として，OpenCVの関数として用意されているCannyのエッジ検出器を使用する．\n","この時，検出されたエッジ画素は255の値を保持しているが，後の処理のために，テンプレート画像のエッジの値を255から1に置き換える．"]},{"cell_type":"code","metadata":{"id":"0oF9HpTXT63h"},"source":["# 使用する画像とテンプレートを用意\n","img = img2.copy()\n","template = img1.copy()\n","\n","# Cannyのエッジ検出器を画像に適用してエッジ検出\n","img_edge = cv2.Canny(img, 150, 500)\n","template_edge = cv2.Canny(template, 150, 500)\n","template_edge[template_edge == 255] = 1 # エッジの画素の値を255 --> 1に変換\n","\n","# エッジの検出結果を表示\n","plt.figure(figsize=(12, 4))\n","plt.subplot(121), plt.imshow(template_edge, cmap='gray', clim=(0, 1))\n","plt.title('Detected Edge of Template')\n","plt.subplot(122), plt.imshow(img_edge, cmap='gray', clim=(0, 255))\n","plt.title('Detected Edge of Input Image')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r_JAP8P6VjYZ"},"source":["次に入力画像における距離変換画像を作成する．\n","\n","ここでは，距離変換画像の作成方法として，`cv2.distanceTransform`を使用する．\n","このとき，`cv2.distanceTransform`の計算では，画素値が0の画素からの距離を算出するため，\n","エッジ画像の画素値を反転させて距離変換画像の作成を行うことに注意されたい．\n"]},{"cell_type":"code","metadata":{"id":"_OMhqSCIVjfD"},"source":["# 距離変換画像の作成\n","img_dist = cv2.distanceTransform(255 - img_edge, cv2.DIST_L1, 0)\n","\n","plt.figure()\n","plt.imshow(img_dist, cmap='gray')\n","plt.colorbar()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iCvDA04KcLm4"},"source":["上記のテンプレート画像のエッジ画像と距離変換画像を用いてチャンファーマッチングによる探索を行う．\n","\n","まず，手動で設定するパラメータとして，チャンファーマッチングにおける更新量のパラメータ`alpha`と初期探索点`(u, v)`を設定する．\n","\n","その後，距離変換画像におけるx, y方向それぞれの勾配を算出する．勾配の算出には，「02. 空間フィルタリング」の微分フィルタを用いる．\n","さらに，テンプレート画像のエッジ画像におけるエッジの点数`n`を算出し，探索途中における探索点を保存するためのリスト`result`を作成する．\n","\n","チャンファーマッチングによる探索では，\n","まずはじめに，距離変換画像とその勾配のうち，現在の探索点`(u, v)`に対応する領域の情報を取得する．\n","次に，相違度`s`の算出を行い，相違度に対する勾配`ds_dx`および`ds_dy`を算出する．\n","算出した勾配を用いて，現在の探索点を更新する．\n","この時，勾配の大きさおよび相違度が閾値以下となった場合に探索を終了する．\n","この条件を満たすまで探索を繰り返すことでマッチングを行う．\n","\n"]},{"cell_type":"code","metadata":{"id":"wUzJx_VCWmm1"},"source":["ht, wt = template_edge.shape\n","\n","# 更新量\n","alpha = 5.0\n","\n","# 初期探索点 (u, v) を指定\n","u, v = 10, 10  # x, y方向\n","\n","# 距離変換画像の勾配算出\n","img_grad_x = img_dist[:, 1:] - img_dist[:, :-1]\n","img_grad_y = img_dist[1:, :] - img_dist[:-1, :]\n","\n","# エッジの点数\n","n = np.sum(template_edge)\n","\n","# 探索途中の結果を保存するためのリストを作成\n","result = [(u, v)]\n","\n","# 探索開始前の座標（初期値）を表示\n","print(\"initial coordinate: (%d, %d)\" % (u, v))\n","\n","# 探索を行うための無限ループ（終了条件は一番下の部分で定義）\n","while True:\n","  img_dist_part = img_dist[v:v+ht, u:u+wt]\n","  grad_x_part = img_grad_x[v:v+ht, u:u+wt]\n","  grad_y_part = img_grad_y[v:v+ht, u:u+wt]\n","\n","  # 相違度sの算出\n","  s = np.sum(template_edge * img_dist_part) / n\n","\n","  # 相違度sの勾配の算出\n","  ds_dx = np.sum(template_edge * grad_x_part) / n\n","  ds_dy = np.sum(template_edge * grad_y_part) / n\n","\n","  # 勾配の大きさと角度を算出\n","  grad_magnitude = math.sqrt(ds_dx**2 + ds_dy**2)\n","  grad_orientation = math.degrees(math.atan(ds_dy / ds_dx))\n","\n","  # 更新\n","  u = u - round(alpha * ds_dx)\n","  v = v - round(alpha * ds_dy)\n","\n","  # 途中結果の保存\n","  result.append((u, v))\n","\n","  # 情報の表示\n","  print(\"u, v: %d, %d\" % (u, v))\n","  print(\"   dx, dy: %0.2f, %0.2f (magnitude: %0.2f, orientation: %0.2f)\" % (ds_dx, ds_dy, grad_magnitude, grad_orientation))\n","  print(\"   s:\", s, \"\\n\")\n","\n","  # 勾配の大きさおよび相違度が閾値以下になった場合に探索を終了する\n","  if grad_magnitude < 0.4 and s < 1.0:\n","    break"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nui44ySIexnr"},"source":["最後に探索した結果を描画する．\n","\n","上の探索中に保存した探索点とそれに対応する範囲の矩形を描画する．\n","ここでは，10回に1回の割合で途中の探索点を描画する．\n","最後に探索結果を白の矩形で描画し，表示を行う．"]},{"cell_type":"code","metadata":{"id":"t58-05lTa9O4"},"source":["# 結果を描画するための画像データをコピーして用意\n","detected = img2.copy()\n","\n","# 探索途中の結果を描画（10回に1回）（グレー）\n","for i in range(0, len(result), 10):\n","  top_left = result[i]\n","  bottom_right = (top_left[0]+wt, top_left[1]+ht)\n","  detected = cv2.rectangle(detected, top_left, bottom_right, 127, 5)\n","\n","# 探索結果（一番最後の探索）を描画（白）\n","top_left = result[-1]\n","bottom_right = (top_left[0]+wt, top_left[1]+ht)\n","detected = cv2.rectangle(detected, top_left, bottom_right, 255, 5)\n","\n","# 描画結果の表示\n","plt.figure()\n","plt.imshow(detected, cmap='gray', clim=(0, 255))\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oLK5van7YX_i"},"source":["## 課題\n","\n","* ヒートマップを観察して，考察すること\n","* 各手法の処理時間を比較し，考察すること\n","* 粗密探索において，低解像度画像のサイズを変更し，処理時間の違いを確認すること"]}]}